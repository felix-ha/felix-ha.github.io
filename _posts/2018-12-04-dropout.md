keep_prob = 0.9

Training neural network:
Epoch: 1 Loss: 14.951968312263489
Epoch: 2 Loss: 14.742331504821777
Epoch: 3 Loss: 14.281008541584015
Epoch: 4 Loss: 13.87766420841217
Epoch: 5 Loss: 13.517071068286896
Epoch: 6 Loss: 13.223572731018066
Epoch: 7 Loss: 12.874436497688293
Epoch: 8 Loss: 12.463171660900116
Epoch: 9 Loss: 12.322922348976135
Epoch: 10 Loss: 11.946803987026215
Epoch: 11 Loss: 11.935391247272491
Epoch: 12 Loss: 11.55950778722763
Epoch: 13 Loss: 11.520590633153915
Epoch: 14 Loss: 11.280707359313965
Epoch: 15 Loss: 11.077617734670639

Optimization Finished!
Test Accuracy: 0.7777778
Validation Accuracy: 0.775
name of prediction:  mlp_model/prediction_op:0
saved model



Testing dropout:
[0.18975161 0.8102483 ]
[0.18975161 0.8102483 ]
[0.18975161 0.8102483 ]
[0.18975161 0.8102483 ]
[0.18975161 0.8102483 ]
mlp_model/dropout_flag:0

---------------
evaluation of training:

Confusion matrix:
[[108  47]
 [ 19 123]]
true negative  | TN FP
true positive  | FN TP
             predicted classes
accuray:  0.7777777777777778
precision:  0.7235294117647059
recall:  0.8661971830985915
f1 score:  0.7884615384615384

saved training summary plot

keep_prob =  0.6

Training neural network:
Epoch: 1 Loss: 17.129649996757507
Epoch: 2 Loss: 16.6850745677948
Epoch: 3 Loss: 16.107725977897644
Epoch: 4 Loss: 15.725143134593964
Epoch: 5 Loss: 15.442670106887817
Epoch: 6 Loss: 15.33218103647232
Epoch: 7 Loss: 15.048871040344238
Epoch: 8 Loss: 14.709356307983398
Epoch: 9 Loss: 14.100905358791351
Epoch: 10 Loss: 13.766059517860413
Epoch: 11 Loss: 13.466299831867218
Epoch: 12 Loss: 13.394644856452942
Epoch: 13 Loss: 13.288953006267548
Epoch: 14 Loss: 12.984191298484802
Epoch: 15 Loss: 12.700531959533691

Optimization Finished!
Test Accuracy: 0.7878788
Validation Accuracy: 0.785
name of prediction:  mlp_model/prediction_op:0
saved model



Testing dropout:
[0.24550432 0.7544957 ]
[0.24550432 0.7544957 ]
[0.24550432 0.7544957 ]
[0.24550432 0.7544957 ]
[0.24550432 0.7544957 ]
mlp_model/dropout_flag:0

---------------
evaluation of training:

Confusion matrix:
[[109  46]
 [ 17 125]]
true negative  | TN FP
true positive  | FN TP
             predicted classes
accuray:  0.7878787878787878
precision:  0.7309941520467836
recall:  0.8802816901408451
f1 score:  0.7987220447284346

saved training summary plot


keep_prob = 0.5

Training neural network:
Epoch: 1 Loss: 17.940826535224915
Epoch: 2 Loss: 17.331783771514893
Epoch: 3 Loss: 16.853139221668243
Epoch: 4 Loss: 16.77691650390625
Epoch: 5 Loss: 16.025052964687347
Epoch: 6 Loss: 15.917295336723328
Epoch: 7 Loss: 15.331414878368378
Epoch: 8 Loss: 15.250825107097626
Epoch: 9 Loss: 14.75502073764801
Epoch: 10 Loss: 14.318027198314667
Epoch: 11 Loss: 14.213816046714783
Epoch: 12 Loss: 13.97406542301178
Epoch: 13 Loss: 13.520296573638916
Epoch: 14 Loss: 13.196354568004608
Epoch: 15 Loss: 12.885477244853973

Optimization Finished!
Test Accuracy: 0.7912458
Validation Accuracy: 0.775
name of prediction:  mlp_model/prediction_op:0
saved model



Testing dropout:
[0.24337211 0.7566279 ]
[0.24337211 0.7566279 ]
[0.24337211 0.7566279 ]
[0.24337211 0.7566279 ]
[0.24337211 0.7566279 ]
mlp_model/dropout_flag:0

---------------
evaluation of training:

Confusion matrix:
[[106  49]
 [ 13 129]]
true negative  | TN FP
true positive  | FN TP
             predicted classes
accuray:  0.7912457912457912
precision:  0.7247191011235955
recall:  0.9084507042253521
f1 score:  0.80625

saved training summary plot


keep_prop = 0.7

Training neural network:
Epoch: 1 Loss: 15.947774410247803
Epoch: 2 Loss: 14.931010663509369
Epoch: 3 Loss: 14.904465794563293
Epoch: 4 Loss: 14.443315029144287
Epoch: 5 Loss: 14.167078733444214
Epoch: 6 Loss: 13.883724629878998
Epoch: 7 Loss: 13.521471500396729
Epoch: 8 Loss: 13.271750032901764
Epoch: 9 Loss: 13.075080335140228
Epoch: 10 Loss: 12.787334740161896
Epoch: 11 Loss: 12.675557136535645
Epoch: 12 Loss: 12.3910071849823
Epoch: 13 Loss: 12.169235050678253
Epoch: 14 Loss: 12.020206689834595
Epoch: 15 Loss: 11.819886028766632

Optimization Finished!
Test Accuracy: 0.7979798
Validation Accuracy: 0.775
name of prediction:  mlp_model/prediction_op:0
saved model



Testing dropout:
[0.22742514 0.7725749 ]
[0.22742514 0.7725749 ]
[0.22742514 0.7725749 ]
[0.22742514 0.7725749 ]
[0.22742514 0.7725749 ]
mlp_model/dropout_flag:0

---------------
evaluation of training:

Confusion matrix:
[[111  44]
 [ 16 126]]
true negative  | TN FP
true positive  | FN TP
             predicted classes
accuray:  0.797979797979798
precision:  0.7411764705882353
recall:  0.8873239436619719
f1 score:  0.8076923076923077

saved training summary plot

keep_prob = 0.8

Training neural network:
Epoch: 1 Loss: 15.65489774942398
Epoch: 2 Loss: 15.29037868976593
Epoch: 3 Loss: 15.159233689308167
Epoch: 4 Loss: 14.837189018726349
Epoch: 5 Loss: 14.538783609867096
Epoch: 6 Loss: 14.156229674816132
Epoch: 7 Loss: 13.566584765911102
Epoch: 8 Loss: 13.57416695356369
Epoch: 9 Loss: 13.218092620372772
Epoch: 10 Loss: 13.083423614501953
Epoch: 11 Loss: 12.77468740940094
Epoch: 12 Loss: 12.394278705120087
Epoch: 13 Loss: 12.575137853622437
Epoch: 14 Loss: 12.028735935688019
Epoch: 15 Loss: 11.753934681415558

Optimization Finished!
Test Accuracy: 0.7811448
Validation Accuracy: 0.775
name of prediction:  mlp_model/prediction_op:0
saved model



Testing dropout:
[0.19742996 0.80257004]
[0.19742996 0.80257004]
[0.19742996 0.80257004]
[0.19742996 0.80257004]
[0.19742996 0.80257004]
mlp_model/dropout_flag:0

---------------
evaluation of training:

Confusion matrix:
[[110  45]
 [ 20 122]]
true negative  | TN FP
true positive  | FN TP
             predicted classes
accuray:  0.7811447811447811
precision:  0.7305389221556886
recall:  0.8591549295774648
f1 score:  0.7896440129449838

saved training summary plot


keep prob = 0.5


Training neural network:
Epoch: 1 Loss: 19.338561177253723
Epoch: 2 Loss: 18.250009894371033
Epoch: 3 Loss: 18.775833427906036
Epoch: 4 Loss: 17.93614000082016
Epoch: 5 Loss: 17.635209321975708
Epoch: 6 Loss: 17.03851729631424
Epoch: 7 Loss: 17.083245277404785
Epoch: 8 Loss: 17.390066027641296
Epoch: 9 Loss: 16.223803460597992
Epoch: 10 Loss: 16.774587392807007
Epoch: 11 Loss: 16.256152033805847
Epoch: 12 Loss: 15.607750117778778
Epoch: 13 Loss: 15.299899220466614
Epoch: 14 Loss: 14.910619020462036
Epoch: 15 Loss: 14.501344203948975
Epoch: 16 Loss: 14.467296421527863
Epoch: 17 Loss: 14.18276184797287
Epoch: 18 Loss: 13.719450831413269
Epoch: 19 Loss: 13.441827237606049
Epoch: 20 Loss: 12.837958216667175
Epoch: 21 Loss: 13.004265487194061
Epoch: 22 Loss: 12.584441840648651
Epoch: 23 Loss: 12.41719776391983
Epoch: 24 Loss: 12.243671029806137
Epoch: 25 Loss: 11.756777971982956
Epoch: 26 Loss: 11.614767640829086
Epoch: 27 Loss: 11.944831013679504
Epoch: 28 Loss: 11.722239673137665
Epoch: 29 Loss: 11.320924013853073
Epoch: 30 Loss: 11.180375337600708
Epoch: 31 Loss: 10.784974694252014
Epoch: 32 Loss: 11.0957772731781
Epoch: 33 Loss: 10.82886216044426
Epoch: 34 Loss: 10.854410111904144
Epoch: 35 Loss: 10.882203727960587
Epoch: 36 Loss: 9.794923603534698
Epoch: 37 Loss: 10.059231668710709
Epoch: 38 Loss: 10.174343705177307
Epoch: 39 Loss: 10.140906780958176
Epoch: 40 Loss: 10.113719195127487
Epoch: 41 Loss: 9.561571180820465
Epoch: 42 Loss: 9.799504339694977
Epoch: 43 Loss: 9.732769906520844
Epoch: 44 Loss: 9.452022790908813
Epoch: 45 Loss: 9.25018247961998
Epoch: 46 Loss: 9.777121245861053
Epoch: 47 Loss: 9.39056220650673
Epoch: 48 Loss: 9.384521633386612
Epoch: 49 Loss: 9.451334297657013
Epoch: 50 Loss: 9.469911217689514
Epoch: 51 Loss: 9.20556265115738
Epoch: 52 Loss: 9.381122708320618
Epoch: 53 Loss: 9.251825124025345
Epoch: 54 Loss: 9.301563501358032
Epoch: 55 Loss: 8.807066768407822
Epoch: 56 Loss: 8.972630053758621
Epoch: 57 Loss: 9.0818589925766
Epoch: 58 Loss: 8.94220644235611
Epoch: 59 Loss: 8.811136901378632
Epoch: 60 Loss: 9.134925335645676
Epoch: 61 Loss: 9.065846055746078
Epoch: 62 Loss: 8.93365329504013
Epoch: 63 Loss: 8.702543467283249
Epoch: 64 Loss: 8.809282004833221
Epoch: 65 Loss: 8.695410460233688
Epoch: 66 Loss: 8.708438038825989
Epoch: 67 Loss: 8.87942698597908
Epoch: 68 Loss: 8.616225451231003
Epoch: 69 Loss: 8.649290174245834
Epoch: 70 Loss: 8.823412507772446
Epoch: 71 Loss: 8.62663358449936
Epoch: 72 Loss: 8.438648223876953
Epoch: 73 Loss: 8.512007147073746
Epoch: 74 Loss: 8.273357480764389
Epoch: 75 Loss: 8.415686845779419
Epoch: 76 Loss: 8.314280450344086
Epoch: 77 Loss: 8.455502420663834
Epoch: 78 Loss: 8.20337650179863
Epoch: 79 Loss: 8.303924351930618
Epoch: 80 Loss: 8.33692780137062
Epoch: 81 Loss: 8.422967612743378
Epoch: 82 Loss: 8.330588310956955
Epoch: 83 Loss: 8.357627749443054
Epoch: 84 Loss: 8.175870716571808
Epoch: 85 Loss: 8.099647015333176
Epoch: 86 Loss: 8.247520446777344
Epoch: 87 Loss: 8.341616868972778
Epoch: 88 Loss: 8.228822588920593
Epoch: 89 Loss: 8.128613144159317
Epoch: 90 Loss: 8.082446575164795
Epoch: 91 Loss: 8.085572063922882
Epoch: 92 Loss: 7.991625964641571
Epoch: 93 Loss: 8.118296682834625
Epoch: 94 Loss: 8.00570234656334
Epoch: 95 Loss: 8.16295912861824
Epoch: 96 Loss: 7.795869052410126
Epoch: 97 Loss: 7.861593544483185
Epoch: 98 Loss: 8.125642120838165
Epoch: 99 Loss: 7.79389876127243
Epoch: 100 Loss: 7.9803891479969025

Optimization Finished!
Test Accuracy: 0.93939394
Validation Accuracy: 0.89
name of prediction:  mlp_model/prediction_op:0
saved model



Testing dropout:
[0.12356131 0.87643874]
[0.12356131 0.87643874]
[0.12356131 0.87643874]
[0.12356131 0.87643874]
[0.12356131 0.87643874]
mlp_model/dropout_flag:0

---------------
evaluation of training:

Confusion matrix:
[[145  10]
 [  8 134]]
true negative  | TN FP
true positive  | FN TP
             predicted classes
accuray:  0.9393939393939394
precision:  0.9305555555555556
recall:  0.9436619718309859
f1 score:  0.9370629370629372

saved training summary plot

dropout = 1

Training neural network:
Epoch: 1 Loss: 14.292652189731598
Epoch: 2 Loss: 13.724289178848267
Epoch: 3 Loss: 13.34444260597229
Epoch: 4 Loss: 13.020650923252106
Epoch: 5 Loss: 12.72943913936615
Epoch: 6 Loss: 12.46122133731842
Epoch: 7 Loss: 12.210135459899902
Epoch: 8 Loss: 11.970184803009033
Epoch: 9 Loss: 11.745222270488739
Epoch: 10 Loss: 11.53679645061493
Epoch: 11 Loss: 11.34135377407074
Epoch: 12 Loss: 11.158303081989288
Epoch: 13 Loss: 10.985885828733444
Epoch: 14 Loss: 10.823524713516235
Epoch: 15 Loss: 10.671306669712067
Epoch: 16 Loss: 10.528869599103928
Epoch: 17 Loss: 10.394491881132126
Epoch: 18 Loss: 10.269748866558075
Epoch: 19 Loss: 10.152264028787613
Epoch: 20 Loss: 10.041645228862762
Epoch: 21 Loss: 9.9368317425251
Epoch: 22 Loss: 9.836985796689987
Epoch: 23 Loss: 9.741144806146622
Epoch: 24 Loss: 9.648889154195786
Epoch: 25 Loss: 9.559920132160187
Epoch: 26 Loss: 9.473672330379486
Epoch: 27 Loss: 9.38995435833931
Epoch: 28 Loss: 9.308428466320038
Epoch: 29 Loss: 9.228989839553833
Epoch: 30 Loss: 9.151423543691635
Epoch: 31 Loss: 9.075823426246643
Epoch: 32 Loss: 9.001917630434036
Epoch: 33 Loss: 8.929930865764618
Epoch: 34 Loss: 8.859439760446548
Epoch: 35 Loss: 8.790279597043991
Epoch: 36 Loss: 8.722716897726059
Epoch: 37 Loss: 8.656826168298721
Epoch: 38 Loss: 8.59254840016365
Epoch: 39 Loss: 8.529816597700119
Epoch: 40 Loss: 8.468924313783646
Epoch: 41 Loss: 8.409506559371948
Epoch: 42 Loss: 8.351384431123734
Epoch: 43 Loss: 8.294529408216476
Epoch: 44 Loss: 8.238629549741745
Epoch: 45 Loss: 8.18401563167572
Epoch: 46 Loss: 8.130408465862274
Epoch: 47 Loss: 8.077770411968231
Epoch: 48 Loss: 8.026184737682343
Epoch: 49 Loss: 7.975367844104767
Epoch: 50 Loss: 7.925888925790787
Epoch: 51 Loss: 7.877417266368866
Epoch: 52 Loss: 7.829584389925003
Epoch: 53 Loss: 7.782479137182236
Epoch: 54 Loss: 7.736260652542114
Epoch: 55 Loss: 7.690671235322952
Epoch: 56 Loss: 7.646082520484924
Epoch: 57 Loss: 7.602595895528793
Epoch: 58 Loss: 7.560280442237854
Epoch: 59 Loss: 7.518135190010071
Epoch: 60 Loss: 7.477479308843613
Epoch: 61 Loss: 7.438045829534531
Epoch: 62 Loss: 7.399887800216675
Epoch: 63 Loss: 7.362599849700928
Epoch: 64 Loss: 7.32600724697113
Epoch: 65 Loss: 7.290082216262817
Epoch: 66 Loss: 7.254678815603256
Epoch: 67 Loss: 7.219780445098877
Epoch: 68 Loss: 7.185383051633835
Epoch: 69 Loss: 7.151678830385208
Epoch: 70 Loss: 7.118349462747574
Epoch: 71 Loss: 7.085249423980713
Epoch: 72 Loss: 7.052474826574326
Epoch: 73 Loss: 7.019433826208115
Epoch: 74 Loss: 6.9875969886779785
Epoch: 75 Loss: 6.956177085638046
Epoch: 76 Loss: 6.9253199100494385
Epoch: 77 Loss: 6.895465970039368
Epoch: 78 Loss: 6.865274876356125
Epoch: 79 Loss: 6.8359712064266205
Epoch: 80 Loss: 6.807486861944199
Epoch: 81 Loss: 6.779433250427246
Epoch: 82 Loss: 6.751680940389633
Epoch: 83 Loss: 6.724380135536194
Epoch: 84 Loss: 6.697588175535202
Epoch: 85 Loss: 6.671268165111542
Epoch: 86 Loss: 6.645143210887909
Epoch: 87 Loss: 6.619501084089279
Epoch: 88 Loss: 6.594103008508682
Epoch: 89 Loss: 6.569208234548569
Epoch: 90 Loss: 6.5447873175144196
Epoch: 91 Loss: 6.520493000745773
Epoch: 92 Loss: 6.49662709236145
Epoch: 93 Loss: 6.472680896520615
Epoch: 94 Loss: 6.449356555938721
Epoch: 95 Loss: 6.426388084888458
Epoch: 96 Loss: 6.4034523367881775
Epoch: 97 Loss: 6.380607485771179
Epoch: 98 Loss: 6.358463704586029
Epoch: 99 Loss: 6.3366924822330475
Epoch: 100 Loss: 6.315365254878998

Optimization Finished!
Test Accuracy: 0.9124579
Validation Accuracy: 0.86
name of prediction:  mlp_model/prediction_op:0
saved model



Testing dropout:
[0.13175969 0.8682403 ]
[0.13175969 0.8682403 ]
[0.13175969 0.8682403 ]
[0.13175969 0.8682403 ]
[0.13175969 0.8682403 ]
mlp_model/dropout_flag:0

---------------
evaluation of training:

Confusion matrix:
[[153   2]
 [ 24 118]]
true negative  | TN FP
true positive  | FN TP
             predicted classes
accuray:  0.9124579124579124
precision:  0.9833333333333333
recall:  0.8309859154929577
f1 score:  0.900763358778626

saved training summary plot